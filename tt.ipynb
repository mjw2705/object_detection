{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "DB_path = './data/VOC2007_trainval'\n",
    "\n",
    "id = 'JPEGImages/000017.jpg'\n",
    "img = cv2.imread(f'{DB_path}/{id}')\n",
    "\n",
    "bnd = '185 62 279 199,90 78 403 336'\n",
    "cls = '14,12'\n",
    "bnd = list(map(str, bnd.split(',')))\n",
    "cls = list(map(int, cls.split(',')))\n",
    "\n",
    "for bbox in bnd:\n",
    "    box = list(map(int, bbox.split(' ')))\n",
    "    img = cv2.rectangle(img, (box[0], box[1]), (box[2], box[3]), (255, 0, 0), 2)\n",
    "\n",
    "cv2.imshow('t', img)\n",
    "cv2.waitKey()\n",
    "cv2.destroyAllWindows()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./data/VOC2007_trainval\\JPEGImages/000012.jpg [[156  97 351 270]] tensor([[0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])\n",
      "./data/VOC2007_trainval\\JPEGImages/000017.jpg [[185  62 279 199]\n",
      " [ 90  78 403 336]] tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "import cv2\n",
    "import torch\n",
    "import pandas as pd\n",
    "\n",
    "DB_path = './data/VOC2007_trainval'\n",
    "output_shape = (416, 416)\n",
    "\n",
    "label_csv = pd.read_csv('2007_train.csv')\n",
    "labels = torch.arange(20)\n",
    "one_hot = torch.nn.functional.one_hot(labels)\n",
    "\n",
    "for idx in range(2):\n",
    "    img_path = os.path.join(f'{DB_path}', label_csv.iloc[idx, 0])\n",
    "    image = cv2.resize(cv2.imread(img_path), output_shape)\n",
    "    image = torch.from_numpy(image).permute(2, 0, 1).float()\n",
    "\n",
    "    bboxs = label_csv.iloc[idx, 1]\n",
    "    bboxs = list(map(str, bboxs.split(',')))\n",
    "    bboxs = np.array(bboxs)\n",
    "    bbox = []\n",
    "    for bx in bboxs:\n",
    "        bbox.append(list(map(int, bx.split(' '))))\n",
    "    bbox = np.array(bbox)\n",
    "\n",
    "    clss = label_csv.iloc[idx, 2]\n",
    "    clss = list(map(int, clss.split(',')))\n",
    "    clss = np.array(clss)\n",
    "    cls = one_hot[clss]\n",
    "    sample = {'image': img_path, 'Bboxs': bbox, 'Classes': clss}\n",
    "\n",
    "    print(img_path, bbox, cls)\n",
    "\n",
    "\n",
    "# labels = torch.arange(20).view(-1, 1)\n",
    "# print(labels)\n",
    "# one_hot = torch.zeros(20, 20).scatter_(1, labels, 1)\n",
    "# print(one_hot)\n",
    "#\n",
    "# one_hot = torch.FloatTensor(20, 20).zero_()\n",
    "# target = one_hot.scatter_(1, labels.data, 1)\n",
    "# print(target)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "anchors_wh = torch.tensor([[10, 13], [16, 30], [33, 23],\n",
    "                           [30, 61], [62, 45], [59, 119],\n",
    "                           [116, 90], [156, 198], [373, 326]]).float() / 416\n",
    "\n",
    "DB_path = './data/VOC2007_trainval'\n",
    "csv_file = '2007_train.csv'\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, num_classes, output_shape=(416, 416)):\n",
    "        self.label_csv = pd.read_csv(f'{csv_file}')\n",
    "        self.num_classes = num_classes\n",
    "        self.output_shape = output_shape\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.label_csv)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        img_path = os.path.join(f'{DB_path}', self.label_csv.iloc[idx, 0])\n",
    "        image = cv2.resize(cv2.imread(img_path), output_shape)\n",
    "        image = torch.from_numpy(image).permute(2, 0, 1).float()\n",
    "\n",
    "        bboxes, classes = self.parse_y_features(idx)\n",
    "\n",
    "        sample = {'image': img_path, 'Bboxs': bboxes, 'Classes': classes}\n",
    "\n",
    "        return sample\n",
    "\n",
    "    def parse_y_features(self, idx):\n",
    "        bboxs = self.label_csv.iloc[idx, 1]\n",
    "        bboxs = list(map(str, bboxs.split(',')))\n",
    "        bboxs = np.array(bboxs)\n",
    "        bbox = []\n",
    "        for bx in bboxs:\n",
    "            bbox.append(list(map(int, bx.split(' '))))\n",
    "        bbox = np.array(bbox)\n",
    "\n",
    "        labels = torch.arange(self.num_classes)\n",
    "        one_hot = torch.nn.functional.one_hot(labels)\n",
    "        clss = self.label_csv.iloc[idx, 2]\n",
    "        clss = list(map(int, clss.split(',')))\n",
    "        clss = np.array(clss)\n",
    "        cls = one_hot[clss]\n",
    "\n",
    "        return bbox, cls\n",
    "\n",
    "    def preprocess_label_for_one_class(self, bbox, classes, grid_size, valid_anchor):\n",
    "        y = torch.zeros((grid_size, grid_size, 3, 5 + self.num_classes))\n",
    "        anchor_idx = self.find_best_anchor(bbox)\n",
    "\n",
    "        num_boxes = classes.shape[0]\n",
    "        for i in range(num_boxes):\n",
    "            curr_class = torch.tensor(classes[i]).float()\n",
    "            curr_box = torch.tensor(bbox[i]).float()\n",
    "            curr_anchor = anchor_idx[i]\n",
    "\n",
    "            # 앵커는 현재 그리드 사이즈에 맞는 것만 사용, 인덱스 3개로 조정\n",
    "            if curr_anchor in valid_anchor:\n",
    "                adjusted_anchor_idx = curr_anchor % 3\n",
    "\n",
    "                # loss 계산을 위해 (xmin, ymin, xmax, ymax)를 (x, y, w, h)로 변환\n",
    "                curr_box_xy = (curr_box[..., 0:2] + curr_box[..., 2:4]) / 2\n",
    "                curr_box_wh = curr_box[..., 2:4] - curr_box[..., 0:2]\n",
    "\n",
    "                # grid cell의 index\n",
    "                grid_cell_xy = curr_box_xy // float(1 / grid_size)\n",
    "                # gird[y][x][anchor] 형태 = (tx, ty, bw, bh, obj, class)\n",
    "                index = torch.tensor([grid_cell_xy[1], grid_cell_xy[0], adjusted_anchor_idx]).int()\n",
    "                update = torch.cat((curr_box_xy, curr_box_wh, torch.tensor([1.0]).float(), curr_class), dim=0)\n",
    "\n",
    "                # gird cell 하나에 대한 anchor 3개\n",
    "                y[index[0]][index[1]][index[2]] = update\n",
    "\n",
    "        return y\n",
    "\n",
    "    def find_best_anchor(self, bbox):\n",
    "\n",
    "        box_wh = torch.from_numpy(bbox[..., 2:4] - bbox[..., 0:2]).float()\n",
    "        box_wh = box_wh.unsqueeze(1).repeat(1, anchors_wh.shape[0], 1).float()  # anchors_wh.shape[0] = 9\n",
    "\n",
    "        intersection = torch.min(box_wh[..., 0], anchors_wh[..., 0]) * torch.min(box_wh[..., 1], anchors_wh[..., 1])\n",
    "        box_area = box_wh[..., 0] * box_wh[..., 1]\n",
    "        anchor_area = anchors_wh[..., 0] * anchors_wh[..., 1]\n",
    "\n",
    "        iou = intersection / (box_area + anchor_area - intersection)\n",
    "        anchor_idx = torch.argmax(iou, dim=-1).int()\n",
    "\n",
    "        return anchor_idx\n",
    "\n",
    "dataset = CustomDataset(20)\n",
    "dataloader = DataLoader(dataset, batch_size=1, shuffle=True, num_workers=0)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2385, 3295,    2], dtype=torch.int32)\n",
      "tensor(253.5000)\n",
      "torch.Size([13, 13, 3, 25])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\mjw27\\anaconda3\\envs\\jw\\lib\\site-packages\\ipykernel_launcher.py:31: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 2385 is out of bounds for dimension 0 with size 13",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mIndexError\u001B[0m                                Traceback (most recent call last)",
      "\u001B[1;32m<ipython-input-215-6f91efb3771f>\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m     78\u001B[0m         \u001B[0mpreprocess_label_for_one_class\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mbboxes\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mclasses\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;36m52\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mtorch\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mtensor\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;36m0\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;36m1\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;36m2\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     79\u001B[0m         \u001B[0mpreprocess_label_for_one_class\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mbboxes\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mclasses\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;36m26\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mtorch\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mtensor\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;36m3\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;36m4\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;36m5\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 80\u001B[1;33m         \u001B[0mpreprocess_label_for_one_class\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mbboxes\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mclasses\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;36m13\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mtorch\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mtensor\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;36m6\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;36m7\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;36m8\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     81\u001B[0m     )\n\u001B[0;32m     82\u001B[0m     \u001B[0msample\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;33m{\u001B[0m\u001B[1;34m'image'\u001B[0m\u001B[1;33m:\u001B[0m \u001B[0mimg_path\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;34m'Bboxs'\u001B[0m\u001B[1;33m:\u001B[0m \u001B[0mbboxes\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;34m'Classes'\u001B[0m\u001B[1;33m:\u001B[0m \u001B[0mclasses\u001B[0m\u001B[1;33m}\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m<ipython-input-215-6f91efb3771f>\u001B[0m in \u001B[0;36mpreprocess_label_for_one_class\u001B[1;34m(bbox, classes, grid_size, valid_anchor)\u001B[0m\n\u001B[0;32m     50\u001B[0m             \u001B[0mprint\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0my\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mshape\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     51\u001B[0m             \u001B[1;31m# gird cell 하나에 대한 anchor 3개\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 52\u001B[1;33m             \u001B[0my\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0mindex\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;36m0\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0mindex\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;36m1\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0mindex\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;36m2\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m]\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mupdate\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     53\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     54\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mIndexError\u001B[0m: index 2385 is out of bounds for dimension 0 with size 13"
     ]
    }
   ],
   "source": [
    "anchors_wh = torch.tensor([[10, 13], [16, 30], [33, 23],\n",
    "                           [30, 61], [62, 45], [59, 119],\n",
    "                           [116, 90], [156, 198], [373, 326]]).float() / 416\n",
    "\n",
    "label_csv = pd.read_csv(f'{csv_file}')\n",
    "\n",
    "def parse_y_features(idx):\n",
    "    bboxs = label_csv.iloc[idx, 1]\n",
    "    bboxs = list(map(str, bboxs.split(',')))\n",
    "    bboxs = np.array(bboxs)\n",
    "    bbox = []\n",
    "    for bx in bboxs:\n",
    "        bbox.append(list(map(int, bx.split(' '))))\n",
    "    bbox = np.array(bbox)\n",
    "\n",
    "    labels = torch.arange(20)\n",
    "    one_hot = torch.nn.functional.one_hot(labels)\n",
    "    clss = label_csv.iloc[idx, 2]\n",
    "    clss = list(map(int, clss.split(',')))\n",
    "    clss = np.array(clss)\n",
    "    cls = one_hot[clss]\n",
    "\n",
    "    return bbox, cls\n",
    "\n",
    "def preprocess_label_for_one_class(bbox, classes, grid_size, valid_anchor):\n",
    "    y = torch.zeros((grid_size, grid_size, 3, 5 + 20))\n",
    "    anchor_idx = find_best_anchor(bbox)\n",
    "\n",
    "    num_boxes = classes.shape[0]\n",
    "    for i in range(num_boxes):\n",
    "        curr_class = torch.tensor(classes[i]).float()\n",
    "        curr_box = torch.tensor(bbox[i]).float()\n",
    "        curr_anchor = anchor_idx[i]\n",
    "\n",
    "        # 앵커는 현재 그리드 사이즈에 맞는 것만 사용, 인덱스 3개로 조정\n",
    "        if curr_anchor in valid_anchor:\n",
    "            adjusted_anchor_idx = curr_anchor % 3\n",
    "\n",
    "            # loss 계산을 위해 (xmin, ymin, xmax, ymax)를 (x, y, w, h)로 변환\n",
    "            curr_box_xy = (curr_box[..., 0:2] + curr_box[..., 2:4]) / 2\n",
    "            curr_box_wh = curr_box[..., 2:4] - curr_box[..., 0:2]\n",
    "\n",
    "            # grid cell의 index\n",
    "            grid_cell_xy = curr_box_xy // float(1 / grid_size)\n",
    "            # gird[y][x][anchor] 형태 = (tx, ty, bw, bh, obj, class)\n",
    "            index = torch.tensor([grid_cell_xy[1], grid_cell_xy[0], adjusted_anchor_idx]).int()\n",
    "            update = torch.cat((curr_box_xy, curr_box_wh, torch.tensor([1.0]).float(), curr_class), dim=0)\n",
    "            print(index)\n",
    "            print(update[0])\n",
    "            print(y.shape)\n",
    "            # gird cell 하나에 대한 anchor 3개\n",
    "            y[index[0]][index[1]][index[2]] = update\n",
    "\n",
    "\n",
    "    return y\n",
    "\n",
    "def find_best_anchor(bbox):\n",
    "    box_wh = torch.from_numpy(bbox[..., 2:4] - bbox[..., 0:2]).float()\n",
    "    box_wh = box_wh.unsqueeze(1).repeat(1, anchors_wh.shape[0], 1).float()  # anchors_wh.shape[0] = 9\n",
    "\n",
    "    intersection = torch.min(box_wh[..., 0], anchors_wh[..., 0]) * torch.min(box_wh[..., 1], anchors_wh[..., 1])\n",
    "    box_area = box_wh[..., 0] * box_wh[..., 1]\n",
    "    anchor_area = anchors_wh[..., 0] * anchors_wh[..., 1]\n",
    "\n",
    "    iou = intersection / (box_area + anchor_area - intersection)\n",
    "    anchor_idx = torch.argmax(iou, dim=-1).int()\n",
    "\n",
    "    return anchor_idx\n",
    "\n",
    "\n",
    "for idx in range(2):\n",
    "    img_path = os.path.join(f'{DB_path}', label_csv.iloc[idx, 0])\n",
    "    image = cv2.resize(cv2.imread(img_path), output_shape)\n",
    "    image = torch.from_numpy(image).permute(2, 0, 1).float()\n",
    "\n",
    "    bboxes, classes = parse_y_features(idx)\n",
    "    label = (\n",
    "        preprocess_label_for_one_class(bboxes, classes, 52, torch.tensor([0, 1, 2])),\n",
    "        preprocess_label_for_one_class(bboxes, classes, 26, torch.tensor([3, 4, 5])),\n",
    "        preprocess_label_for_one_class(bboxes, classes, 13, torch.tensor([6, 7, 8]))\n",
    "    )\n",
    "    sample = {'image': img_path, 'Bboxs': bboxes, 'Classes': classes}\n",
    "\n",
    "print(bboxes, classes)\n",
    "\n",
    "# print(sample)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}